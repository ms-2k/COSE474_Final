%%%%%%%% ICML 2019 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2019} with \usepackage[nohyperref]{icml2019} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2019}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2019}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{COSE474-2023F: Final Project Proposal}

\begin{document}

\twocolumn[
\icmltitle{COSE474-2021F: Final Project \linebreak
           Machine Generated Journalism Detection Using Deep Learning}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2019
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Minseo Kim}{}
\end{icmlauthorlist}

%\icmlaffiliation{ku}{Department of Computer Science \& Engineering, Korea University, Seoul, Korea}


%\icmlcorrespondingauthor{the}{myemail@korea.ac.kr}
%\icmlcorrespondingauthor{Eee Pppp}{ep@eden.co.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

%\begin{abstract}
%This document provides a basic paper template and submission guidelines.
%Abstracts must be a single paragraph, ideally between 4--6 sentences long.
%Gross violations will trigger corrections at the camera-ready phase.
%\end{abstract}

\section{Introduction}
\quad With the advent of generative large language models it has become easier to generate journalism with little human intervention.
However, this could potentially be dangerous for numerous reasons, such as plagiarism, proliferation of misinformation, and misuse.
The purpose of this work is to train a model capable of distinguishing human written journalism from machine generated journalism with transformer based architecture.
In particular, this project will focus on jounalism in Korean, where the topic is relatively less explored. \linebreak
\null\quad The source code to this project can be found in the following github repository:
\begin{center}
    \url{https://github.com/ms-2k/COSE474_Final}
\end{center}
\null\quad However, while the dataset was generated from publicly available data, to adhear to distribution rights they are not included in the github repository.

\section{Motivation}
\null\quad In an era of highly sophisticated LLMs, there is a real danger of misuse of machine generated journalism.
Research has shown that it is difficult to distinguish between human generated content and machine generated content, especially if the latter is generated by paraphrasing human generated content. (Jiang, G.)
There is real danger of bad actors mass generating and distributing believable machine generated fake news. \linebreak
\null\quad Furthermore, while there are plenty of machine generated content detection services in English, few are trained on Korean corpora.
Thus, there is a necessity to train a model capable of distinguishing human generated content from machine generated content in Korean.

\section{Problem Definition}
\null\quad The goal of this project is to train a model capable of distinguishing human written news from machine generated news with transformer based architecture.
The model should be capable of accepting any article written in Korean, and output a binary label indicating whether the article is human generated or not.
However, as available computing resources are too limited to train the model from scratch, the final model should be fine tuned from an existing BERT based model.

\section{Contribution}
\null\quad The sole main contributor to this project is Minseo Kim.

\section{Related Works and Baseline}
\null\quad While there are few studies on detecting machine generated Korean journalism, similar attempts have been made by numerous researchers to detect AI generated content in general.
Recent works such as RADAR (Hu et al., 2023) and DetectGPT (Mitchell et al., 2023) have attempted to distinguish human and machine generated text,
where the former in particular utilized an adversarial model with human generated text along with machine generated text generated and paraphrased from the human written text. \linebreak
\null\quad However, neither of these works are trained on Korean corpora.
Thus for this project we will be using various foundation models as the baseline to see if our fine tuned model is better.

\section{Method}

\section{Significance \& Novelty}

\section{Overall Structure}

\section{Experiments}

\section{Dataset}

\section{Computing Resources}

\section{Experiment Setup}

\section{Comparison with Baseline}

\section{Conclusion}

\bibliographystyle{unsrt}
\bibliography{references}
Jiang, G. (2023, May 30). Is AI-generated content actually detectable?: College of Computer, Mathematical, and Natural Sciences: University of Maryland. College of Computer, Mathematical, and Natural Sciences | University of Maryland. https://cmns.umd.edu/news-events/news/ai-generated-content-actually-detectable 

Hu, X., Chen, P. Y., \& Ho, T. Y. (2023). RADAR: Robust AI-Text Detection via Adversarial Learning. arXiv preprint arXiv:2307.03838.

Mitchell, E., Lee, Y., Khazatsky, A., Manning, C. D., \& Finn, C. (2023). Detectgpt: Zero-shot machine-generated text detection using probability curvature. arXiv preprint arXiv:2301.11305.

kr-FinBert-SC, Kim, Eunhee and Hyopil Shin (2022). KR-FinBert: Fine-tuning KR-FinBert for Sentiment Analysis. huggingface \url{https://huggingface.co/snunlp/KR-FinBert-SC}
\end{document}