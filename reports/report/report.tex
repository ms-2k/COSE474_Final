%%%%%%%% ICML 2019 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2019} with \usepackage[nohyperref]{icml2019} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2019}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2019}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{COSE474-2023F: Final Project Proposal}

\begin{document}

\twocolumn[
\icmltitle{COSE474-2021F: Final Project \linebreak
           Machine Generated Journalism Detection Using Deep Learning}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2019
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Minseo Kim}{}
\end{icmlauthorlist}

%\icmlaffiliation{ku}{Department of Computer Science \& Engineering, Korea University, Seoul, Korea}


%\icmlcorrespondingauthor{the}{myemail@korea.ac.kr}
%\icmlcorrespondingauthor{Eee Pppp}{ep@eden.co.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

%\begin{abstract}
%This document provides a basic paper template and submission guidelines.
%Abstracts must be a single paragraph, ideally between 4--6 sentences long.
%Gross violations will trigger corrections at the camera-ready phase.
%\end{abstract}

\section{Introduction}
\quad In the recent years, Large Language Models (LLMs) have shown a remarkable ability to generate human-like text, making them a potentially valuable tool for automated journalism.
The quality of text generated by these advanced artificial intelligence systems has improved to such an extent that it has become virtually indistinguishable from text authored by humans (Jiang, G.).
The intricacies of human language patterns, nuances, and semantics that were once considered exclusive to human cognition are now being effectively replicated by these advanced artificial intelligence models.
This raises compelling discussions about the implications on authorship and the authenticity of text in the digital age.
However, this same capability also poses several significant risks. \\
\null\quad Firstly, the widespread use of LLMs in journalism could lead to an increase in misinformation.
Given that these models generate content based on the data they are trained on,
they are susceptible to replicating and amplifying any biases present in that data.
This could potentially lead to the creation and dissemination of biased or misleading news articles. \\
\null\quad Secondly, the use of LLMs in journalism raises ethical concerns around plagiarism.
Since these models are trained on large quantities of data,
it is possible that they could inadvertently generate text that closely resembles existing articles,
violating intellectual property rights. \\
\null\quad Lastly, the advent of LLMs could potentially undermine public trust in journalism.
If audiences are unable to distinguish between human and machine generated content,
it could lead to a general distrust in the information they consume,
further exacerbating the current crisis of misinformation. \\
\null\quad On the other hand, while several models exist to classify English articles as human-written or machine generated,
the Korean language presents unique linguistic and structural facets which these models may not effectively capture.
Therefore, there is a pressing need to specifically train a model capable of classifying Korean articles.
This would contribute to ensuring the veracity of journalism in Korean language and help to maintain the integrity of information consumed by the public. \\
\null\quad To tackle this problem, this project aims to train a model capable of distinguishing human written and machine generated text.
In particular, the project aims to train a model capable of distinguishing human written and machine generated text in Korean, where the topic is relatively less explored. \\

\section{Source Code}
\null\quad The source code to this project can be found in the following github repository:
\begin{center} \url{https://github.com/ms-2k/COSE474_Final} \end{center}
\null\quad However, while the dataset was generated from publicly available data, to adhear to distribution rights they are not included in the github repository.

\section{Problem Definition}
\null\quad The primary objective of this research endeavor is to develop a computational model that can accurately differentiate between news articles authored by humans and those generated by machines, utilizing a transformer-based architecture.
The intended model should possess the capability to analyze and interpret any given news article text written in the Korean language and subsequently produce a binary output.
This binary output, in the form of a label, will indicate whether the given article is a product of human intellect or a result of machine generation. \\
\null\quad Despite the ambitious nature of this project, it is important to acknowledge the constraint of limited available computational resources.
Therefore, to overcome this limitation and ensure efficient use of resources, the final model will not be built from scratch.
Instead, we intend to fine-tune an existing model based on the DistilBERT (Distilled Bidirectional Encoder Representations from Transformers) architecture.
This approach leverages the established capabilities of DistilBERT while allowing us to tailor the model to our specific task of distinguishing between human-written and machine-generated news articles in Korean.

\section{Contribution}
\null\quad The sole main contributor to this project is Minseo Kim.

\section{Related Works and Baseline}
\null\quad While there are few studies on detecting machine generated Korean journalism, similar attempts have been made by numerous researchers to detect AI generated content in general.
Recent works such as RADAR (Hu et al., 2023) and DetectGPT (Mitchell et al., 2023) have attempted to distinguish human and machine generated text,
where the former in particular utilized an adversarial model with human generated text along with machine generated text generated and paraphrased from the human written text. \\
\null\quad Regrettably, these preceding works, while innovative in their own rights, have not been trained on Korean corpora.
This presents a significant challenge when attempting to utilize them as a baseline for comparison with our model.
The linguistic nuances and specificities inherent to the Korean language render these models less applicable in the context of our research.
Consequently, in order to establish a more suitable benchmark for evaluation, we have opted to use other existing AI text detection services, such as ZeroGPT and Smodin.

\section{Methodology}
\null\quad In the preliminary stages of model development, we encountered two predominant challenges that necessitated resolution.
The first challenge pertained to the acquisition of a sufficiently large dataset encompassing both human-authored journalism and machine-created journalistic content.
This issue was further exacerbated when our focus was narrowed to Korean corpora specifically, due to its limited availability. \\
\null\quad The second challenge was the notable disproportion in the amount of publicly accessible human-authored journalism compared to its machine-generated counterpart.
This discrepancy in data distribution could potentially culminate in a class imbalance problem,
thereby predisposing the model towards predicting news articles as being human-authored. \\
\null\quad To effectively navigate these challenges, we embarked on the creation of our own datasets.
This was accomplished by sourcing publicly available Korean news online and subsequently paraphrasing these articles with the assistance of one open source, and one closed sourcec Large Language Models with text-to-text generation capabilities.
This approach enabled us to compile a dataset with an equal representation of human-authored and machine-generated news.
It also held the potential to create a dataset of adequate size to facilitate the training of a robust model. \\
\null\quad Upon successful compilation of the dataset, we initiated the process of model development.
Given the constraints on computational resources, we made the strategic decision to leverage a pre-existing DistilBERT model, rather than constructing a new one from scratch. \\
\null\quad Ultimately, an isolated segment of the dataset, earmarked specifically for testing purposes, was employed to gauge the performance of our fine-tuned model.
For the sake of comparison, these tests were also conducted on baseline models using a similarly sized testing dataset,
thereby allowing us to juxtapose the effectiveness of our model against existing frameworks.

\section{Overall Structure}
*insert image here later*

\section{Experiments}
\null\quad The development of our model entailed the execution of numerous experiments, systematically conducted across three distinct stages.
The initial stage, focused on dataset acquisition, involved testing several LLMs known for their text generation capabilities.
Our main objective at this stage was to identify which combination of models and hyperparameters would lead to the generation of text most closely resembling human-authored content.
Among the models subjected to testing were base LLaMA 2, beomi's Korean fine-tuned version of LLaMA 2, OpenChat 3.5, Google's T5, Google's Flan-T5, and GPT3.5.
Notably, due to limitations in our computational resources, we confined our experiments to the 7B and 13B models.
We further explored the influence of various hyperparameters on the output, including temperature, top\_p, and repetition\_penalty.
Our findings from these experiments indicated that the most optimal configuration for our specific needs comprised of the OpenChat 3.5 and GPT3.5 models, with a temperature setting of 0.7 and a top\_p value of 0.8. \\

*experiment with other text classification base models later*

\section{Dataset}
\null\quad As elucidated in the preceding sections, the dataset under consideration is constituted by an aggregation of news articles that are readily accessible to the public and originate from a variety of Korean news media outlets.
In tandem with these human-authored articles, the dataset also encompasses articles that have been machine generated.
It is imperative to note that these machine-generated articles are not original creations of the machine, but rather, they are paraphrased versions of the human-authored articles. \\
\null\quad This paraphrasing task was executed by a select group of LLMs that demonstrate proficiency in text-to-text generation.
The majority of the paraphrased articles were processed using the OpenChat 3.5 model, which was operated locally.
This choice of execution was predominantly guided by the financial constraints that were in play, which led to limited funds being available for conducting API calls to the OpenAI API GPT-3.5 with high frequency.

\section{Computing Resources}
Our experiments were conducted utilizing the following computing resources:

\subsection{Computer 1}
\begin{itemize}
    \item Central Processing Unit (CPU): AMD Ryzen 5800X
    \item Graphics Processing Unit (GPU): Nvidia RTX 3060Ti with 6GB VRAM
    \item Random Access Memory (RAM): 24GB
\end{itemize}

\subsection{Computer 2}
\begin{itemize}
    \item Central Processing Unit (CPU): Intel i7 4790
    \item Graphics Processing Unit (GPU): Nvidia GT730
    \item Random Access Memory (RAM): 24GB
\end{itemize}

Computer 1 was utilized for all stages of development, while Computer 2 was occasionally used to perform API calls to the OpenAI API during the dataset collection stage.

\section{Experiment Setup}

\section{Comparison with Baseline}

\section{Conclusion}

\bibliographystyle{unsrt}
\bibliography{references}
Jiang, G. (2023, May 30). Is AI-generated content actually detectable?: College of Computer, Mathematical, and Natural Sciences: University of Maryland. College of Computer, Mathematical, and Natural Sciences | University of Maryland. https://cmns.umd.edu/news-events/news/ai-generated-content-actually-detectable 

Hu, X., Chen, P. Y., \& Ho, T. Y. (2023). RADAR: Robust AI-Text Detection via Adversarial Learning. arXiv preprint arXiv:2307.03838.

Mitchell, E., Lee, Y., Khazatsky, A., Manning, C. D., \& Finn, C. (2023). Detectgpt: Zero-shot machine-generated text detection using probability curvature. arXiv preprint arXiv:2301.11305.

kr-FinBert-SC, Kim, Eunhee and Hyopil Shin (2022). KR-FinBert: Fine-tuning KR-FinBert for Sentiment Analysis. huggingface \url{https://huggingface.co/snunlp/KR-FinBert-SC}
\end{document}